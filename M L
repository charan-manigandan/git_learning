Creating Tensors:
           string=tf.Variable("This is a string", tf.string)
           syntax : var_name = tf.Variable("data or object", tf.data_type)

Rank/Degree of Tensors:
           rank1_tensor = tf.Variable(['test','ok'], tf.string)
           rank2_tensor = tf.Variable([["test","ok"],["some","nothing"]], tf.string)
            tf.rank(rank2_tensor)
            ----> used to find the nested list in data or object given in TF

shape of tensors:
              returns a list of frequency of data available in total data
              rank2_tensor.shape

changing shape:
	tensor1 = tf.ones([1,2,3])  # creates a tensor full of ones
	tensor2 = tf reshape(tensor1, [2,3,1]) # reshapes the tensor1 like [2,3,1]
	tensor3 = tf.reshape(tensor2, [3, -1]) # gives a shape format [3,2]


Types of Tensors:
	-->Variable
	-->Constant
	-->PlaceHolder
	-->SparseTensor

Evaluating Tensors:
	To evaluate tf we need a session()

	with tf.Session() as session_name: #creates session using the default graph
		tensor_name.eval()


sample data below :
dftrain=pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
dfeval=pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')
y_train = dftrain.pop('survived')
y_eval = dfeval.pop('survived')
print(y_train)

if we want to locate any specific row use data_name.loc[index]

data_name.describe() -->  more statisttical analysis of given data

data_name.row_name.hist(condtn) -->  plots a histogram with given data

data_name.row_name.value_count().plot() --> to count and pot a bar graph

pd.comcat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh).set_xlabel('% survive)



The Training ProcessSo we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. 
Specifically how input data is fed to our model.For this specific model data is going to be streamed into it in small batches of 32. 

This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will actually feed these batches to our
model multiple times according to the number of epochs.An epoch is simply one stream of our entire dataset. The number of epochs we define is the amount 
of times our model will see the entire dataset. 

We use multiple epochs in hope after seeing the same data multiple times the model will better 
determine how to estimate it.Ex. if we have 10 ephocs, our model will see the same dataset 10 times.Since we need to feed our data in batches 
and multiple times we need to create something called an input function. The input function simply defines how our dataset will be converted 
into batches at each epoch.

Input Function
	The TensorFlow model we are going to use requires that the data we pass it comes in as a tf.data.Dataset object. 
  This means we must create a input function that can convert our current pandas dataframe into that object.Below you'll see a seemingly complicated input function, 
  this is straight from the TensorFlow documentation(https://www.tensorflow.org/tutorials/estimator/linear). I've commented as much as I can to make it understandble, 
  but you may want to refer to the documentation for a detailed explination of each method.
  
  def make_input_fn(data_df, label_df, num_epochs -10, shuffle-True, batch_size=32):
  	def input function(): # inner function, this will be returned 
		ds=tf.data.Dataset.from tensor_slices((dict(data_df), label_df)) # create tf.data.Dataset object with data and its la 
		if shuffle: 	ds=ds.shuffle(1000) # randomize order of data 
		ds=ds.batch(batch_size).repeat(num_epochs) # split dataset into batches of 32 and repeat process for number of epocs epoc 
		return ds # return a batch of the dataset 
	return input_function #return a function object for use 

train_input_fn = make_input_fn(dftrain, y_train) # here we will call the input function that was returned
eval_input_fn make_input_fn(dfeval, y_eval, num_epochs 1, shuffle=False)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)

linear_est.train(train_input_fn) # train
result = linear_est.evaluate(eval_input_fn) # get model metrics/stats by testing on tetsing data
clear output() #clears consoke output 
print(result['accuracy']) # the result variable is simply a dict of stats about our model
				
NOTE : TO GET MORE ACCURACY OF THE TRAINED MODEL CHANGE THE EPOCHS NOR CLEAN THE DATA MORE FURTHER
	As the Data gets shuffled for each time to get understaning of data we get a change always in output
				
result = list(linear_est.predict(eval_input_fn))
print(dfeval.loc[2])  # this loc tf will give us the details present in the list . eg: sex, age, siblings, fare, class, etc.
print(y_eval.loc[2])
print(result[0]['probabilities'][1])	 # prints the survival rate of the above trained data			
				
Classification
	We've covered linear regression it is time to talk about classification. Where regression was used to predict a numeric value, classification is used to 
	seperate data points into classes of different labels. In this example we will use a TensorFlow estimator to classifyflowers.Since we've touched on how 
	estimators work earlier I'll go a bit quicker through this example. 
	This section is based on the following guide from the TensorFlow website. https://www.tensorflow.org/tutorials/estimator/premade
	
Imports and Setup

[21] Stensorflow version 2.x this line is not required unless you are in a notebook

[22] from __future__ isport absolute import, division, print function, unicode literals
import tensorflow as tf
import pandas as pd

Dataset
This specific dataset seperates flowers into 3 different classes of species.
⚫Setosa
⚫Versicolor
⚫Virginica

The information about each flower is the following.
⚫ sepal length
⚫ sepal width
⚫petal length
⚫petal width

[ ] CSV_COLUMN_NAMES= ['SepalLength', 'Sepalwidth', 'PetalLength', 'Petalwidth', 'Species'] 
SPECIES = [Setosa', 'Versicolor', 'Virginica'] 
#Lets define some constants to help us later on

[] train_path tf.keras.utils.get_file("iris_training.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv")
test_path=tf.keras.utils.get_file("iris_test.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv")
train = pd.read_csv(train_path, names-CSV_COLUMN_NAMES, header=0) 
test= pd.read_csv(test_path, names CSV_COLUMN_NAMES, header=0)
Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe

[] train.head()

train_ytrain.pop('Species') 
test_y test.pop('Species')
]train.head() #the species column is now gone

[] train.shape()

Input Function
	Remember that nasty input function we created earlier. Well we need to make another one here! Fortunatly for us this one is a little easier to digest.

[] def input_fn(features, labels, training-True, batch_size=256): # Convert the inputs to a Dataset. 
	dataset= tf.data.Dataset.from_tensor_slices ((dict (features), labels))#Shuffle and repeat if you are in training mode.
	    if training :  dataset= dataset.shuffle(1000).repeat()
	        return dataset.batch(batch_size)
		
Feature Columns
[] # Feature columns describe how to use the input.
my_feature_columns = []
for key in train.keys():
    my_feature_columns.append(tf.feature_column.numeric_column (key-key)) 
        print (my_feature_columns)


